# Вклад у проект: Етап аналізу даних (Data Analysis Stage)

**Автор:** Oskar Oleksandr Kobel  
**Етап:** Data Analysis Stage

## Огляд

Цей документ описує реалізацію етапу аналізу даних та підготовки проекту до навчання Machine Learning моделей. Реалізовано інфраструктуру для ML-пайплайну, інтеграцію з існуючими модулями трансформації та покращення архітектури проекту.

## Що було реалізовано

### 1. Реструктуризація проекту для ML-пайплайну

**Мета:** Організувати проект у модульну структуру, яка підтримує як етап трансформації, так і етап ML-аналізу.

**Створена структура:**

```
anime-spark-project/
├── transformation/          # Етап трансформації даних
│   ├── data_extraction.py
│   ├── business_questions.py
│   ├── dataset_info.py
│   └── numeric_statistics.py
├── data_analysis/          # Етап ML-аналізу (НОВИЙ)
│   ├── prepare_ml_datasets.py
│   ├── prepare_classification_dataset.py
│   ├── regression_modeling.py
│   ├── classification_modeling.py
│   └── ML_DATASETS_DATA_CARD.md
└── main.py                 # Оновлено для підтримки обох етапів
```

**Переваги нової структури:**
- Чітке розділення відповідальності між етапами
- Легке додавання нових ML-моделей
- Модульність та підтримуваність коду
- Можливість незалежного розвитку кожного етапу

### 2. Інтеграція ML-пайплайну в головний меню

**Модуль:** `main.py`

**Реалізовано:**

1. **Інтерактивне меню з вибором етапів:**
   - Бізнес-питання (трансформація + зірчаста схема)
   - Регресія ML (підготовка датасету + моделі)
   - Класифікація ML (підготовка датасету + моделі)

2. **Автоматична перевірка та підготовка даних:**
   - `ensure_raw_data()` - перевірка наявності сирих CSV файлів
   - `ensure_star_schema()` - створення/завантаження зірчастої схеми
   - `ensure_regression_dataset()` - підготовка датасету для регресії
   - `ensure_classification_dataset()` - підготовка датасету для класифікації

3. **Оптимізація виконання:**
   - Перевірка наявності проміжних результатів (Parquet файли)
   - Пропуск повторних обчислень, якщо дані вже існують
   - Логічне об'єднання етапів пайплайну

**Приклад використання:**

```python
# Автоматична перевірка та підготовка
ensure_raw_data(data_path)
dim_user, dim_anime, dim_date, fact_ratings = ensure_star_schema(spark, data_path)

# Виконання бізнес-питань
run_business_questions_flow(spark, data_path)

# Підготовка ML-датасетів
ensure_regression_dataset(spark, data_path)
ensure_classification_dataset(spark, data_path)
```

### 3. Підготовка інфраструктури для ML-моделей

**Модулі:** `data_analysis/regression_modeling.py`, `data_analysis/classification_modeling.py`

**Реалізовано:**

1. **Заглушки для ML-моделей:**
   - `run_regression_modeling()` - інтеграція регресійних моделей
   - `run_classification_modeling()` - інтеграція класифікаційних моделей
   - Підготовка до додавання реальних моделей навчання

2. **Інтеграція з підготовкою датасетів:**
   - Автоматична перевірка наявності підготовлених датасетів
   - Завантаження train/validation/test розділів
   - Збереження метаданих про підготовку

3. **Структура для збереження результатів:**
   - `data/ml_datasets/regression/` - датасети для регресії
   - `data/ml_datasets/classification/` - датасети для класифікації
   - `preprocessing_info.json` - метадані про підготовку
   - `split_info.json` - інформація про розділення датасету

### 4. Покращення Docker інфраструктури

**Файли:** `run_docker_windows.bat`, `run_docker_mac_linux.sh`

**Реалізовано крос-платформенну підтримку:**

1. **Windows скрипт (`run_docker_windows.bat`):**
   - Автоматична збірка Docker образу
   - Правильне монтування Windows-шляхів
   - Інтерактивний режим з паузою

2. **Mac/Linux скрипт (`run_docker_mac_linux.sh`):**
   - Автоматична збірка Docker образу
   - Підтримка змінної середовища `IMAGE_NAME`
   - Bash-сумісний синтаксис

**Переваги:**
- Один командний запуск на будь-якій платформі
- Автоматична обробка відсутніх образів
- Збереження результатів на хост-машині

## Технічні деталі

### Архітектура пайплайну

```
Raw Data (CSV)
    ↓
Star Schema Creation (Parquet)
    ↓
Business Questions Analysis
    ↓
ML Dataset Preparation
    ├── Regression Dataset
    └── Classification Dataset
    ↓
Model Training (заглушки готові)
```

### Використані технології

- **PySpark** - для обробки великих датасетів
- **Docker** - для крос-платформенної підтримки
- **Parquet** - для ефективного зберігання проміжних результатів
- **JSON** - для метаданих та конфігурації

### Оптимізації

1. **Кешування проміжних результатів:**
   - Зірчаста схема зберігається в Parquet
   - ML-датасети зберігаються після підготовки
   - Перевірка наявності перед повторним обчисленням

2. **Модульність:**
   - Кожен етап може виконуватися незалежно
   - Легке додавання нових моделей
   - Чіткі інтерфейси між модулями

## Структура проекту після реструктуризації

```
anime-spark-project/
├── data/                    # Дані (CSV, Parquet, ML datasets)
│   ├── star_schema/         # Зірчаста схема (Parquet)
│   ├── ml_datasets/         # ML-датасети
│   │   ├── regression/
│   │   └── classification/
│   └── results/             # Результати аналізу
├── transformation/          # Модулі трансформації
├── data_analysis/           # Модулі ML-аналізу
├── main.py                  # Головний файл з меню
├── Dockerfile               # Docker конфігурація
├── run_docker_windows.bat   # Скрипт запуску (Windows)
└── run_docker_mac_linux.sh  # Скрипт запуску (Mac/Linux)
```

## Результати

### Досягнуто:

1. ✅ Модульна структура проекту з чітким розділенням етапів
2. ✅ Інтеграція ML-пайплайну в головне меню
3. ✅ Автоматична підготовка та перевірка даних
4. ✅ Крос-платформенна підтримка Docker
5. ✅ Інфраструктура для додавання ML-моделей
6. ✅ Оптимізація через кешування проміжних результатів

### Готовність до навчання моделей:

- ✅ Підготовка датасетів для регресії та класифікації
- ✅ Розділення на train/validation/test
- ✅ Збереження метаданих про підготовку
- ✅ Інтеграція в головний пайплайн
- ✅ Заглушки для моделей готові до наповнення

## Висновки

Реалізовано повну інфраструктуру для переходу від етапу трансформації до етапу ML-аналізу. Проект тепер має:

1. **Модульну архітектуру** - легко додавати нові моделі та етапи
2. **Автоматизацію** - мінімум ручної роботи при запуску
3. **Крос-платформенність** - однакова робота на Windows, Mac та Linux
4. **Оптимізацію** - кешування проміжних результатів для швидшого виконання
5. **Готовність до ML** - інфраструктура готова для додавання реальних моделей навчання

Всі зміни зберегли сумісність з існуючим кодом та покращили підтримуваність проекту.