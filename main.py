import os
from typing import Tuple

from transformation.data_extraction import (
    create_star_schema,
    save_star_schema_to_parquet,
    load_star_schema_from_parquet,
)
from transformation.business_questions import (
    run_artem_questions,
    run_bohdan_questions,
    run_oskar_questions,
    run_arii_extended_questions,
)
from transformation.dataset_info import run_dataset_info_analysis
from transformation.numeric_statistics import run_numeric_statistics_analysis
from transformation.raw_data_extraction import run_raw_data_extraction
from data_analysis.prepare_ml_datasets import (
    prepare_regression_dataset,
    prepare_classification_dataset,
    split_and_save,
)
from data_analysis.classification_modeling import run_classification_modeling
from data_analysis.regression_modeling import run_regression_modeling

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ PySpark
try:
    from pyspark.sql import SparkSession, DataFrame  # type: ignore

    SPARK_AVAILABLE = True
except ImportError:
    SparkSession = None  # type: ignore
    DataFrame = None  # type: ignore
    SPARK_AVAILABLE = False


DATA_PATH = "data"


def create_spark_session() -> "SparkSession":
    """
    –°—Ç–≤–æ—Ä—é—î SparkSession –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É.
    """
    assert SPARK_AVAILABLE, "PySpark –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"
    spark = (
        SparkSession.builder.appName("AnimeSparkApp")
        .config(
            "spark.driver.extraJavaOptions",
            "--add-opens=java.base/java.nio=ALL-UNNAMED "
            "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED",
        )
        .config(
            "spark.executor.extraJavaOptions",
            "--add-opens=java.base/java.nio=ALL-UNNAMED "
            "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED",
        )
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
        .config("spark.sql.parquet.datetimeRebaseModeInWrite", "LEGACY")
        .config("spark.driver.memory", "2g")
        .config("spark.executor.memory", "2g")
        .config("spark.sql.execution.arrow.pyspark.enabled", "false")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.sql.execution.arrow.maxRecordsPerBatch", "1000")
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("ERROR")
    return spark


# ======================================================================
# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞–¥—ñ–π –ø–∞–π–ø–ª–∞–π–Ω—É
# ======================================================================


def ensure_raw_data(data_path: str = DATA_PATH) -> None:
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Å–∏—Ä–∏—Ö CSV —Ñ–∞–π–ª—ñ–≤.
    –Ø–∫—â–æ —è–∫–∏—Ö–æ—Å—å –Ω–µ –≤–∏—Å—Ç–∞—á–∞—î ‚Äì –∑–∞–ø—É—Å–∫–∞—î –∑–∞–≥–ª—É—à–∫—É raw data extraction.
    """
    required_files = [
        "anime-dataset-2023.csv",
        "users-details-2023.csv",
        "users-score-2023.csv",
    ]
    missing = [
        f for f in required_files if not os.path.exists(os.path.join(data_path, f))
    ]

    if not missing:
        print("‚úÖ –°–∏—Ä—ñ –¥–∞–Ω—ñ –≤–∂–µ –ø—Ä–∏—Å—É—Ç–Ω—ñ (CSV —Ñ–∞–π–ª–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ).")
        return

    print("‚ö†Ô∏è –ù–µ –≤–∏—Å—Ç–∞—á–∞—î —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö:")
    for name in missing:
        print(f"   - {name}")
    print("‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫–∞—é –∑–∞–≥–ª—É—à–∫—É —Å—Ç–∞–¥—ñ—ó 'data extraction'...")
    run_raw_data_extraction(data_path=data_path, missing_files=missing)


def _star_schema_parquet_exists(data_path: str = DATA_PATH) -> bool:
    base = os.path.join(data_path, "star_schema")
    required_dirs = [
        "dim_user",
        "dim_anime",
        "dim_date",
        "fact_user_ratings",
    ]
    return all(os.path.exists(os.path.join(base, d)) for d in required_dirs)


def ensure_star_schema(
    spark: "SparkSession", data_path: str = DATA_PATH
) -> Tuple["DataFrame", "DataFrame", "DataFrame", "DataFrame"]:
    """
    –ì–∞—Ä–∞–Ω—Ç—É—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏:
    - —è–∫—â–æ —î Parquet ‚Äì –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î
    - —ñ–Ω–∞–∫—à–µ —Å—Ç–≤–æ—Ä—é—î –∑ –Ω—É–ª—è —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –≤ Parquet
    """
    parquet_path = os.path.join(data_path, "star_schema")

    if _star_schema_parquet_exists(data_path):
        print("\nüìÇ –ó—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞ –≤–∂–µ —ñ—Å–Ω—É—î –≤ Parquet ‚Äì –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é...")
        return load_star_schema_from_parquet(spark, parquet_path=parquet_path)

    print("\nüåü –ó—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞ —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–∞ ‚Äì —Å—Ç–≤–æ—Ä—é—é –∑ –Ω—É–ª—è...")
    dim_user, dim_anime, dim_date, fact_ratings = create_star_schema(
        spark, data_path=data_path
    )
    save_star_schema_to_parquet(
        dim_user, dim_anime, dim_date, fact_ratings, output_path=parquet_path
    )
    return dim_user, dim_anime, dim_date, fact_ratings


def _ml_dataset_meta_path(kind: str, data_path: str = DATA_PATH) -> str:
    return os.path.join(data_path, "ml_datasets", kind, "preprocessing_info.json")


def ensure_regression_dataset(spark: "SparkSession", data_path: str = DATA_PATH) -> None:
    """
    –ì–∞—Ä–∞–Ω—Ç—É—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å ML –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î PySpark-–ø–∞–π–ø–ª–∞–π–Ω –∑ data_analysis.prepare_ml_datasets.
    """
    meta_path = _ml_dataset_meta_path("regression", data_path)
    output_dir = os.path.dirname(meta_path)

    if os.path.exists(meta_path):
        print("‚úÖ ML –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–≥—Ä–µ—Å—ñ—ó –≤–∂–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–π.")
        return

    print("\nüì¶ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ ML –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó...")
    os.makedirs(output_dir, exist_ok=True)

    regression_df = prepare_regression_dataset(
        spark,
        anime_path=os.path.join(data_path, "anime-filtered.csv"),
        output_path=output_dir,
    )

    split_and_save(
        regression_df,
        output_dir,
        stratify_column=None,
        format_type="both",
    )
    print("‚úÖ ML –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ.")


def ensure_classification_dataset(
    spark: "SparkSession", data_path: str = DATA_PATH
) -> None:
    """
    –ì–∞—Ä–∞–Ω—Ç—É—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å ML –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î PySpark-–ø–∞–π–ø–ª–∞–π–Ω –∑ data_analysis.prepare_ml_datasets.
    """
    meta_path = _ml_dataset_meta_path("classification", data_path)
    output_dir = os.path.dirname(meta_path)

    if os.path.exists(meta_path):
        print("‚úÖ ML –¥–∞—Ç–∞—Å–µ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –≤–∂–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–π.")
        return

    print("\nüì¶ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ ML –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó...")
    os.makedirs(output_dir, exist_ok=True)

    classification_df = prepare_classification_dataset(
        spark,
        ratings_path=os.path.join(data_path, "users-score-2023.csv"),
        users_path=os.path.join(data_path, "users-details-2023.csv"),
        anime_path=os.path.join(data_path, "anime-filtered.csv"),
        output_path=output_dir,
        min_ratings=10,
        sample_users=50000,
    )

    split_and_save(
        classification_df,
        output_dir,
        stratify_column="gender_encoded",
        format_type="both",
    )
    print("‚úÖ ML –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ.")


# ======================================================================
# –°—Ü–µ–Ω–∞—Ä—ñ—ó / —Å—Ç–∞–¥—ñ—ó, —è–∫—ñ –º–æ–∂–Ω–∞ –∑–∞–ø—É—Å–∫–∞—Ç–∏ –∑ –º–µ–Ω—é
# ======================================================================


def run_business_questions_flow(spark: "SparkSession", data_path: str = DATA_PATH) -> None:
    """
    –ü–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –±—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω—å:
    - –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö
    - —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è / –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏
    - –±–∞–∑–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–∞—Ç–∞—Å–µ—Ç—É (dataset_info + numeric_statistics)
    - –∑–∞–ø—É—Å–∫ —É—Å—ñ—Ö –±—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω—å (Artem, Bohdan, Oskar)
    """
    ensure_raw_data(data_path)
    dim_user, dim_anime, dim_date, fact_ratings = ensure_star_schema(
        spark, data_path=data_path
    )

    print("\n" + "=" * 60)
    print("üîÑ –¢–†–ê–ù–°–§–û–†–ú–ê–¶–Ü–Ø –¢–ê –ë–ê–ó–û–í–ò–ô –ê–ù–ê–õ–Ü–ó –î–ê–ù–ò–•")
    print("=" * 60)

    anime_dataset_path = os.path.join(data_path, "anime-dataset-2023.csv")
    df_anime_original = spark.read.csv(anime_dataset_path, header=True, inferSchema=True)

    run_dataset_info_analysis(
        df_anime_original,
        output_dir=os.path.join(data_path, "results"),
    )

    run_numeric_statistics_analysis(
        df_anime_original,
        output_dir=os.path.join(data_path, "results"),
    )

    print("\n" + "=" * 60)
    print("‚ùì –ë–Ü–ó–ù–ï–°-–ü–ò–¢–ê–ù–ù–Ø")
    print("=" * 60)

    results_path = os.path.join(data_path, "results")

    run_artem_questions(fact_ratings, dim_user, dim_anime, dim_date, results_path)
    run_bohdan_questions(fact_ratings, dim_user, dim_anime, dim_date, results_path)
    run_oskar_questions(fact_ratings, dim_user, dim_anime, dim_date, results_path)
    run_arii_extended_questions(fact_ratings, dim_user, dim_anime, dim_date, results_path)

    print("\n‚úÖ –£—Å—ñ –±—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω–Ω—è –≤–∏–∫–æ–Ω–∞–Ω–æ.")


def run_prepare_all_ml_datasets(spark: "SparkSession", data_path: str = DATA_PATH) -> None:
    """
    –û–∫—Ä–µ–º–∏–π —Å—Ü–µ–Ω–∞—Ä—ñ–π: –ø—Ä–∏–º—É—Å–æ–≤–æ –ø–µ—Ä–µ–±—É–¥—É–≤–∞—Ç–∏ –æ–±–∏–¥–≤–∞ ML –¥–∞—Ç–∞—Å–µ—Ç–∏.
    """
    ensure_raw_data(data_path)
    ensure_regression_dataset(spark, data_path=data_path)
    ensure_classification_dataset(spark, data_path=data_path)


def run_regression_step(spark: "SparkSession", data_path: str = DATA_PATH) -> None:
    """
    –°—Ç–∞–¥—ñ—è: –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ) + –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ.
    
    –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ - –ø—Ä–∞—Ü—é—î –Ω–∞–ø—Ä—è–º—É –∑ —Å–∏—Ä–∏–º–∏ CSV.
    """
    ensure_raw_data(data_path)
    ensure_regression_dataset(spark, data_path=data_path)
    run_regression_modeling(data_path=data_path)


def run_classification_step(spark: "SparkSession", data_path: str = DATA_PATH) -> None:
    """
    –°—Ç–∞–¥—ñ—è: –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ) + –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ.
    
    –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ - –ø—Ä–∞—Ü—é—î –Ω–∞–ø—Ä—è–º—É –∑ —Å–∏—Ä–∏–º–∏ CSV.
    """
    ensure_raw_data(data_path)
    ensure_classification_dataset(spark, data_path=data_path)
    run_classification_modeling(data_path=data_path)


# ======================================================================
# CLI-–º–µ–Ω—é
# ======================================================================


def print_menu() -> None:
    print("\n" + "=" * 60)
    print("üéõ  Anime Spark ‚Äì –≥–æ–ª–æ–≤–Ω–µ –º–µ–Ω—é")
    print("=" * 60)
    print("1) –ë—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω–Ω—è (–ø–æ—Ç—Ä–µ–±—É—î: —Å–∏—Ä—ñ –¥–∞–Ω—ñ ‚Üí –∑—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞)")
    print("   ‚îî‚îÄ –°—Ç–≤–æ—Ä—é—î –∑—ñ—Ä—á–∞—Å—Ç—É —Å—Ö–µ–º—É, –∑–∞–ø—É—Å–∫–∞—î –∞–Ω–∞–ª—ñ–∑ —Ç–∞ –±—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω–Ω—è")
    print("")
    print("2) –†–µ–≥—Ä–µ—Å—ñ—è ML (–ø–æ—Ç—Ä–µ–±—É—î: —Å–∏—Ä—ñ –¥–∞–Ω—ñ ‚Üí ML –¥–∞—Ç–∞—Å–µ—Ç —Ä–µ–≥—Ä–µ—Å—ñ—ó)")
    print("   ‚îî‚îÄ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ) + –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª—ñ")
    print("")
    print("3) –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ML (–ø–æ—Ç—Ä–µ–±—É—î: —Å–∏—Ä—ñ –¥–∞–Ω—ñ ‚Üí ML –¥–∞—Ç–∞—Å–µ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)")
    print("   ‚îî‚îÄ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ) + –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª—ñ")
    print("")
    print("0) –í–∏—Ö—ñ–¥")


def main() -> None:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ç–æ—á–∫–∞ –≤—Ö–æ–¥—É –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É.

    –î–æ–∑–≤–æ–ª—è—î –æ–±—Ä–∞—Ç–∏ —Å—Ç–∞–¥—ñ—é –ø–∞–π–ø–ª–∞–π–Ω—É:
    - –±—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω–Ω—è
    - —Ä–µ–≥—Ä–µ—Å—ñ—è (ML, –∑–∞–≥–ª—É—à–∫–∞)
    - –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (ML, –∑–∞–≥–ª—É—à–∫–∞)
    """
    if not SPARK_AVAILABLE:
        print("‚ùå PySpark –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Docker –¥–ª—è –∑–∞–ø—É—Å–∫—É.")
        print('   –ó–∞–ø—É—Å—Ç—ñ—Ç—å: docker run -v "$(pwd)/data:/app/data" my-spark-img')
        return

    print("üöÄ –ó–∞–ø—É—Å–∫ Anime Spark App (PySpark)")

    spark = create_spark_session()

    try:
        while True:
            print_menu()
            choice = input("–û–±–µ—Ä—ñ—Ç—å –¥—ñ—é: ").strip()

            if choice == "1":
                run_business_questions_flow(spark, data_path=DATA_PATH)
            elif choice == "2":
                run_regression_step(spark, data_path=DATA_PATH)
            elif choice == "3":
                run_classification_step(spark, data_path=DATA_PATH)
            elif choice == "0":
                print("üëã –í–∏—Ö—ñ–¥ –∑ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É.")
                break
            else:
                print("‚ö†Ô∏è –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.")
    finally:
        spark.stop()


if __name__ == "__main__":
    main()