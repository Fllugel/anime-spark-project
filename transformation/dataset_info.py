"""
Dataset Information Module

This module provides functions to extract and describe general information
about the anime dataset including schema, row count, column count, and column names.
"""

import json
import os
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, isnull, count as spark_count


def get_dataset_info(df: DataFrame) -> dict:
    """
    –û—Ç—Ä–∏–º—É—î –∑–∞–≥–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏

    Returns:
        Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö:
        - row_count: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤
        - column_count: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ–≤–ø—Ü—ñ–≤
        - columns: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ —Å—Ç–æ–≤–ø—Ü—ñ–≤
        - schema: —Å—Ö–µ–º–∞ –¥–∞—Ç–∞—Å–µ—Ç—É
        - null_counts: –∫—ñ–ª—å–∫—ñ—Å—Ç—å null –∑–Ω–∞—á–µ–Ω—å –ø–æ –∫–æ–∂–Ω–æ–º—É —Å—Ç–æ–≤–ø—Ü—é
    """
    print("\n" + "="*60)
    print("–ï–¢–ê–ü 1: –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö")
    print("="*60)
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –±–∞–∑–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
    row_count = df.count()
    columns = df.columns
    column_count = len(columns)
    
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ö–µ–º—É
    schema_dict = []
    for field in df.schema.fields:
        schema_dict.append({
            'name': field.name,
            'type': str(field.dataType),
            'nullable': field.nullable
        })
    
    # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ null –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å—Ç–æ–≤–ø—Ü—è
    print("\n–ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ null –∑–Ω–∞—á–µ–Ω—å...")
    null_counts = {}
    for col_name in columns:
        null_count = df.filter(isnull(col(col_name))).count()
        null_counts[col_name] = null_count
    
    info = {
        'row_count': row_count,
        'column_count': column_count,
        'columns': columns,
        'schema': schema_dict,
        'null_counts': null_counts
    }
    
    return info


def describe_dataset(df: DataFrame) -> dict:
    """
    –û–ø–∏—Å—É—î –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –æ—Ç—Ä–∏–º–∞–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏

    Returns:
        Dict –∑ –æ–ø–∏—Å–æ–º –¥–∞—Ç–∞—Å–µ—Ç—É
    """
    info = get_dataset_info(df)
    
    print(f"\nüìä –û–ü–ò–° –ù–ê–ë–û–†–£ –î–ê–ù–ò–•:")
    print(f"  ‚Ä¢ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤: {info['row_count']:,}")
    print(f"  ‚Ä¢ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ–≤–ø—Ü—ñ–≤: {info['column_count']}")
    
    print(f"\nüìã –°–¢–û–í–ü–¶–Ü ({info['column_count']}):")
    for i, col_name in enumerate(info['columns'], 1):
        null_count = info['null_counts'][col_name]
        null_percentage = (null_count / info['row_count'] * 100) if info['row_count'] > 0 else 0
        print(f"  {i:2d}. {col_name:20s} - {null_count:6d} null ({null_percentage:5.2f}%)")
    
    print(f"\nüìê –°–•–ï–ú–ê –î–ê–¢–ê–°–ï–¢–£:")
    for field in info['schema']:
        nullable_str = "nullable" if field['nullable'] else "not nullable"
        print(f"  ‚Ä¢ {field['name']:20s}: {field['type']:30s} ({nullable_str})")
    
    # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥ –¥–∞–Ω–∏—Ö
    print(f"\nüîç –ü–†–ò–ö–õ–ê–î –î–ê–ù–ò–• (–ø–µ—Ä—à—ñ 3 —Ä—è–¥–∫–∏):")
    df.show(3, truncate=50)
    
    return info


def save_dataset_info(info: dict, output_dir: str = "data/results") -> str:
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç —É JSON —Ñ–∞–π–ª.

    Args:
        info: Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

    Returns:
        –®–ª—è—Ö –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "dataset_info.json")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å—Ö–µ–º—É —É JSON-—Å—É–º—ñ—Å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
    json_info = {
        'row_count': info['row_count'],
        'column_count': info['column_count'],
        'columns': info['columns'],
        'schema': info['schema'],
        'null_counts': info['null_counts']
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(json_info, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {output_path}")
    return output_path


def run_dataset_info_analysis(df: DataFrame, output_dir: str = "data/results") -> dict:
    """
    –ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

    Returns:
        Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç
    """
    info = describe_dataset(df)
    save_dataset_info(info, output_dir)
    return info

