"""
Numeric Statistics Module

This module provides functions to calculate and analyze statistics
for numeric columns in the anime dataset.
"""

import json
import os
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, mean, stddev, min as spark_min, max as spark_max, count, isnull
from pyspark.sql.types import NumericType


def get_numeric_columns(df: DataFrame) -> list:
    """
    –û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤ —É DataFrame.

    Args:
        df: Spark DataFrame

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
    """
    numeric_cols = []
    for field in df.schema.fields:
        if isinstance(field.dataType, NumericType):
            numeric_cols.append(field.name)
    return numeric_cols


def get_numeric_statistics(df: DataFrame) -> dict:
    """
    –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —â–æ–¥–æ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏

    Returns:
        Dict –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–≤–ø—Ü—è:
        - count: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–Ω—É–ª—å–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        - mean: —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è
        - stddev: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è
        - min: –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
        - max: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
    """
    print("\n" + "="*60)
    print("–ï–¢–ê–ü 2: –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —â–æ–¥–æ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤")
    print("="*60)
    
    numeric_cols = get_numeric_columns(df)
    
    if not numeric_cols:
        print("‚ö†Ô∏è  –ß–∏—Å–ª–æ–≤—ñ —Å—Ç–æ–≤–ø—Ü—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ!")
        return {}
    
    print(f"\n–ó–Ω–∞–π–¥–µ–Ω–æ {len(numeric_cols)} —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤: {', '.join(numeric_cols)}")
    
    statistics = {}
    
    for col_name in numeric_cols:
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Å—Ç–æ–≤–ø—Ü—è '{col_name}':")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –±–∞–∑–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_df = df.select(
            count(col(col_name)).alias('count'),
            mean(col(col_name)).alias('mean'),
            stddev(col(col_name)).alias('stddev'),
            spark_min(col(col_name)).alias('min'),
            spark_max(col(col_name)).alias('max')
        )
        
        stats_row = stats_df.collect()[0]
        
        col_stats = {
            'count': stats_row['count'],
            'mean': float(stats_row['mean']) if stats_row['mean'] is not None else None,
            'stddev': float(stats_row['stddev']) if stats_row['stddev'] is not None else None,
            'min': float(stats_row['min']) if stats_row['min'] is not None else None,
            'max': float(stats_row['max']) if stats_row['max'] is not None else None
        }
        
        # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ null –∑–Ω–∞—á–µ–Ω–Ω—è
        null_count = df.filter(isnull(col(col_name)) | col(col_name).isNull()).count()
        total_count = df.count()
        col_stats['null_count'] = null_count
        col_stats['null_percentage'] = (null_count / total_count * 100) if total_count > 0 else 0
        
        statistics[col_name] = col_stats
        
        # –í–∏–≤–æ–¥–∏–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"  ‚Ä¢ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞—á–µ–Ω—å: {col_stats['count']:,}")
        print(f"  ‚Ä¢ Null –∑–Ω–∞—á–µ–Ω—å: {col_stats['null_count']:,} ({col_stats['null_percentage']:.2f}%)")
        if col_stats['mean'] is not None:
            print(f"  ‚Ä¢ –°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è: {col_stats['mean']:.2f}")
        if col_stats['stddev'] is not None:
            print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è: {col_stats['stddev']:.2f}")
        if col_stats['min'] is not None:
            print(f"  ‚Ä¢ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è: {col_stats['min']:.2f}")
        if col_stats['max'] is not None:
            print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è: {col_stats['max']:.2f}")
    
    return statistics


def analyze_numeric_columns(df: DataFrame, statistics: dict = None) -> dict:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç—å –∞–Ω–∞–ª—ñ–∑ –æ—Ç—Ä–∏–º–∞–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —á–∏—Å–ª–æ–≤—ñ —Å—Ç–æ–≤–ø—Ü—ñ.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏
        statistics: Dict –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é (—è–∫—â–æ None, –±—É–¥–µ –æ–±—á–∏—Å–ª–µ–Ω–æ)

    Returns:
        Dict –∑ –∞–Ω–∞–ª—ñ–∑–æ–º —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
    """
    if statistics is None:
        statistics = get_numeric_statistics(df)
    
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–Ü–ó –ß–ò–°–õ–û–í–ò–• –°–¢–û–í–ü–¶–Ü–í")
    print("="*60)
    
    analysis = {}
    
    for col_name, stats in statistics.items():
        print(f"\nüîç –ê–Ω–∞–ª—ñ–∑ —Å—Ç–æ–≤–ø—Ü—è '{col_name}':")
        
        col_analysis = {
            'data_quality': {},
            'distribution': {},
            'insights': []
        }
        
        # –ê–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
        null_pct = stats['null_percentage']
        if null_pct == 0:
            quality = "–í—ñ–¥–º—ñ–Ω–Ω–∞ - –Ω–µ–º–∞—î –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å"
        elif null_pct < 5:
            quality = "–î–æ–±—Ä–∞ - –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å"
        elif null_pct < 20:
            quality = "–ü—Ä–∏–π–Ω—è—Ç–Ω–∞ - –ø–æ–º—ñ—Ä–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å"
        else:
            quality = "–ü–æ–≥–∞–Ω–∞ - –±–∞–≥–∞—Ç–æ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å"
        
        col_analysis['data_quality']['null_percentage'] = null_pct
        col_analysis['data_quality']['assessment'] = quality
        print(f"  ‚Ä¢ –Ø–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö: {quality} ({null_pct:.2f}% null)")
        
        # –ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
        if stats['mean'] is not None and stats['stddev'] is not None and stats['stddev'] > 0:
            cv = (stats['stddev'] / stats['mean']) * 100  # –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤–∞—Ä—ñ–∞—Ü—ñ—ó
            col_analysis['distribution']['coefficient_of_variation'] = cv
            
            if cv < 15:
                dist_assessment = "–ù–∏–∑—å–∫–∞ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å - –¥–∞–Ω—ñ –¥–æ—Å–∏—Ç—å –æ–¥–Ω–æ—Ä—ñ–¥–Ω—ñ"
            elif cv < 35:
                dist_assessment = "–ü–æ–º—ñ—Ä–Ω–∞ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å - –¥–∞–Ω—ñ –º–∞—é—Ç—å —Å–µ—Ä–µ–¥–Ω—é —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å"
            else:
                dist_assessment = "–í–∏—Å–æ–∫–∞ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å - –¥–∞–Ω—ñ –¥—É–∂–µ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ"
            
            col_analysis['distribution']['assessment'] = dist_assessment
            print(f"  ‚Ä¢ –†–æ–∑–ø–æ–¥—ñ–ª: {dist_assessment} (CV: {cv:.2f}%)")
        
        # –î—ñ–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω—å
        if stats['min'] is not None and stats['max'] is not None:
            range_val = stats['max'] - stats['min']
            col_analysis['distribution']['range'] = range_val
            col_analysis['distribution']['min'] = stats['min']
            col_analysis['distribution']['max'] = stats['max']
            print(f"  ‚Ä¢ –î—ñ–∞–ø–∞–∑–æ–Ω: –≤—ñ–¥ {stats['min']:.2f} –¥–æ {stats['max']:.2f} (—Ä–æ–∑–º–∞—Ö: {range_val:.2f})")
        
        # –Ü–Ω—Å–∞–π—Ç–∏
        insights = []
        if stats['mean'] is not None:
            insights.append(f"–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è: {stats['mean']:.2f}")
        if stats['stddev'] is not None and stats['stddev'] > 0:
            insights.append(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è: {stats['stddev']:.2f}")
        
        col_analysis['insights'] = insights
        analysis[col_name] = col_analysis
    
    return analysis


def save_numeric_statistics(statistics: dict, analysis: dict, output_dir: str = "output/results"):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–∞ –∞–Ω–∞–ª—ñ–∑ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤ —É —Ñ–∞–π–ª–∏.

    Args:
        statistics: Dict –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
        analysis: Dict –∑ –∞–Ω–∞–ª—ñ–∑–æ–º
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ JSON –∑ –ø–æ–≤–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é
    json_path = os.path.join(output_dir, "numeric_statistics.json")
    json_data = {
        'statistics': statistics,
        'analysis': analysis
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É JSON: {json_path}")
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ CSV –∑ –±–∞–∑–æ–≤–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
    csv_path = os.path.join(output_dir, "numeric_statistics.csv")
    import csv
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Column', 'Count', 'Mean', 'StdDev', 'Min', 'Max', 'Null Count', 'Null %'])
        
        for col_name, stats in statistics.items():
            writer.writerow([
                col_name,
                stats['count'],
                stats['mean'] if stats['mean'] is not None else '',
                stats['stddev'] if stats['stddev'] is not None else '',
                stats['min'] if stats['min'] is not None else '',
                stats['max'] if stats['max'] is not None else '',
                stats['null_count'],
                f"{stats['null_percentage']:.2f}"
            ])
    
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É CSV: {csv_path}")


def run_numeric_statistics_analysis(df: DataFrame, output_dir: str = "output/results") -> tuple:
    """
    –ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤.

    Args:
        df: Spark DataFrame –∑ –∞–Ω—ñ–º–µ –¥–∞–Ω–∏–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

    Returns:
        Tuple (statistics, analysis)
    """
    statistics = get_numeric_statistics(df)
    analysis = analyze_numeric_columns(df, statistics)
    save_numeric_statistics(statistics, analysis, output_dir)
    return statistics, analysis

