"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ –¥–∞–Ω–∏—Ö (Star Schema) –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∞–Ω—ñ–º–µ –¥–∞—Ç–∞—Å–µ—Ç—É.
–°—Ç–≤–æ—Ä—é—î –≤–∏–º—ñ—Ä–∏ (Dimensions) —Ç–∞ —Ç–∞–±–ª–∏—Ü—é —Ñ–∞–∫—Ç—ñ–≤ (Fact Table) –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É.
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, IntegerType, StringType, DoubleType, 
    DateType, BooleanType, FloatType
)
from pyspark.sql.functions import (
    col, when, isnull, lit, row_number, to_date, year, quarter, month,
    dayofweek, date_format, expr, monotonically_increasing_id
)
from pyspark.sql.window import Window


def create_dim_user(spark: SparkSession, users_details_path: str):
    """
    –°—Ç–≤–æ—Ä—é—î –≤–∏–º—ñ—Ä –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (Dim_User) –∑ —Å—É—Ä–æ–≥–∞—Ç–Ω–∏–º –∫–ª—é—á–µ–º.
    
    Args:
        spark: SparkSession
        users_details_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É users-details-2023.csv
        
    Returns:
        DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
    """
    print("üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Dim_User...")
    
    # –ó—á–∏—Ç—É—î–º–æ –¥–∞–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
    df_users = spark.read.csv(users_details_path, header=True, inferSchema=True)
    
    # –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ —Å—Ö–µ–º—ñ
    df_users = df_users.select(
        col("Mal ID").alias("User_ID"),
        col("Username").alias("Username"),
        col("Gender").alias("Gender"),
        col("Birthday").alias("Birthday"),
        col("Location").alias("Location"),
        col("Joined").alias("Joined_Date"),
        col("Mean Score").alias("User_Mean_Score"),
        col("Completed").alias("User_Total_Completed"),
        col("Watching").alias("User_Watching"),
        col("On Hold").alias("User_On_Hold"),
        col("Dropped").alias("User_Dropped"),
        col("Plan to Watch").alias("User_Plan_to_Watch"),
        col("Total Entries").alias("User_Total_Entries"),
        col("Days Watched").alias("User_Days_Watched"),
        col("Episodes Watched").alias("User_Episodes_Watched")
    )
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –¥–∞—Ç–∏ (–æ–±—Ä–æ–±–ª—è—î–º–æ —Ñ–æ—Ä–º–∞—Ç ISO –∑ —á–∞—Å–æ–≤–∏–º –ø–æ—è—Å–æ–º)
    from pyspark.sql.functions import regexp_replace, split
    
    # –í–∏–¥–∞–ª—è—î–º–æ —á–∞—Å–æ–≤–∏–π –ø–æ—è—Å —Ç–∞ —á–∞—Å –∑ –¥–∞—Ç (—Ñ–æ—Ä–º–∞—Ç: 2011-01-10T00:00:00+00:00 -> 2011-01-10)
    df_users = df_users.withColumn(
        "Birthday", 
        when(col("Birthday").isNotNull(), 
             to_date(regexp_replace(col("Birthday"), "T.*", ""), "yyyy-MM-dd"))
        .otherwise(None)
    )
    df_users = df_users.withColumn(
        "Joined_Date",
        when(col("Joined_Date").isNotNull(),
             to_date(regexp_replace(col("Joined_Date"), "T.*", ""), "yyyy-MM-dd"))
        .otherwise(None)
    )
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å—É—Ä–æ–≥–∞—Ç–Ω–∏–π –∫–ª—é—á (User_SK)
    window = Window.orderBy("User_ID")
    df_users = df_users.withColumn("User_SK", row_number().over(window))
    
    # –í–∏–±–µ—Ä–∞—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
    dim_user = df_users.select(
        "User_SK",
        "User_ID",
        "Username",
        "Gender",
        "Birthday",
        "Location",
        "Joined_Date",
        "User_Mean_Score",
        "User_Total_Completed",
        "User_Watching",
        "User_On_Hold",
        "User_Dropped",
        "User_Plan_to_Watch",
        "User_Total_Entries",
        "User_Days_Watched",
        "User_Episodes_Watched"
    )
    
    print(f"‚úÖ Dim_User —Å—Ç–≤–æ—Ä–µ–Ω–æ: {dim_user.count()} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤")
    return dim_user


def create_dim_anime(spark: SparkSession, anime_dataset_path: str):
    """
    –°—Ç–≤–æ—Ä—é—î –≤–∏–º—ñ—Ä –∞–Ω—ñ–º–µ (Dim_Anime) –∑ —Å—É—Ä–æ–≥–∞—Ç–Ω–∏–º –∫–ª—é—á–µ–º.
    
    Args:
        spark: SparkSession
        anime_dataset_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É anime-dataset-2023.csv
        
    Returns:
        DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∞–Ω—ñ–º–µ
    """
    print("üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Dim_Anime...")
    
    # –ó—á–∏—Ç—É—î–º–æ –¥–∞–Ω—ñ –∞–Ω—ñ–º–µ
    df_anime = spark.read.csv(anime_dataset_path, header=True, inferSchema=True)
    
    # –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î–º–æ —Ç–∞ –≤–∏–±–∏—Ä–∞—î–º–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
    df_anime = df_anime.select(
        col("anime_id").alias("Anime_ID"),
        col("Name").alias("Name"),
        col("English name").alias("English_Name"),
        col("Type").alias("Type"),
        col("Source").alias("Source"),
        col("Genres").alias("Genres"),
        col("Studios").alias("Studios"),
        col("Producers").alias("Producers"),
        col("Score").alias("Avg_Score"),
        col("Popularity").alias("Popularity_Rank"),
        col("Episodes").alias("Episodes"),
        col("Rating").alias("Age_Rating"),
        col("Rank").alias("Rank"),
        col("Members").alias("Members"),
        col("Favorites").alias("Favorites"),
        col("Scored By").alias("Scored_By"),
        col("Aired").alias("Aired"),
        col("Premiered").alias("Premiered"),
        col("Status").alias("Status"),
        col("Duration").alias("Duration")
    )
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å—É—Ä–æ–≥–∞—Ç–Ω–∏–π –∫–ª—é—á (Anime_SK)
    window = Window.orderBy("Anime_ID")
    df_anime = df_anime.withColumn("Anime_SK", row_number().over(window))
    
    # –í–∏–±–µ—Ä–∞—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
    dim_anime = df_anime.select(
        "Anime_SK",
        "Anime_ID",
        "Name",
        "English_Name",
        "Type",
        "Source",
        "Genres",
        "Studios",
        "Producers",
        "Avg_Score",
        "Popularity_Rank",
        "Episodes",
        "Age_Rating",
        "Rank",
        "Members",
        "Favorites",
        "Scored_By",
        "Aired",
        "Premiered",
        "Status",
        "Duration"
    )
    
    print(f"‚úÖ Dim_Anime —Å—Ç–≤–æ—Ä–µ–Ω–æ: {dim_anime.count()} –∞–Ω—ñ–º–µ")
    return dim_anime


def create_dim_date(spark: SparkSession, start_date: str = "2000-01-01", end_date: str = "2025-12-31"):
    """
    –°—Ç–≤–æ—Ä—é—î –≤–∏–º—ñ—Ä –¥–∞—Ç–∏ (Dim_Date) –∑ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–µ–Ω–¥—ñ–≤.
    
    Args:
        spark: SparkSession
        start_date: –ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç: 'YYYY-MM-DD')
        end_date: –ö—ñ–Ω—Ü–µ–≤–∞ –¥–∞—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç: 'YYYY-MM-DD')
        
    Returns:
        DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –¥–∞—Ç–∏
    """
    print("üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Dim_Date...")
    
    # –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –¥–∞—Ç —á–µ—Ä–µ–∑ SQL (–¥–ª—è Spark 3.0+)
    try:
        df_dates = spark.sql(f"""
            SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as Full_Date
        """)
    except:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Å–ø–æ—Å—ñ–± –¥–ª—è —Å—Ç–∞—Ä—ñ—à–∏—Ö –≤–µ—Ä—Å—ñ–π Spark
        from datetime import datetime, timedelta
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        dates = []
        current = start
        while current <= end:
            dates.append((current.date(),))
            current += timedelta(days=1)
        
        from pyspark.sql.types import StructType, StructField, DateType
        schema = StructType([StructField("Full_Date", DateType(), True)])
        df_dates = spark.createDataFrame(dates, schema)
    
    # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ –¥–∞—Ç–∏
    dim_date = df_dates.select(
        col("Full_Date"),
        year(col("Full_Date")).alias("Year"),
        quarter(col("Full_Date")).alias("Quarter"),
        month(col("Full_Date")).alias("Month"),
        date_format(col("Full_Date"), "MMMM").alias("Month_Name"),
        date_format(col("Full_Date"), "EEEE").alias("Day_of_Week"),
        # –í Spark dayofweek: 1=Sunday, 7=Saturday
        when(dayofweek(col("Full_Date")).isin([1, 7]), True).otherwise(False).alias("Is_Weekend")
    )
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å—É—Ä–æ–≥–∞—Ç–Ω–∏–π –∫–ª—é—á (Date_SK) —É —Ñ–æ—Ä–º–∞—Ç—ñ YYYYMMDD
    dim_date = dim_date.withColumn(
        "Date_SK",
        expr("cast(date_format(Full_Date, 'yyyyMMdd') as int)")
    )
    
    # –í–∏–±–µ—Ä–∞—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
    dim_date = dim_date.select(
        "Date_SK",
        "Full_Date",
        "Year",
        "Quarter",
        "Month",
        "Month_Name",
        "Day_of_Week",
        "Is_Weekend"
    )
    
    print(f"‚úÖ Dim_Date —Å—Ç–≤–æ—Ä–µ–Ω–æ: {dim_date.count()} –¥–∞—Ç")
    return dim_date


def create_fact_user_ratings(
    spark: SparkSession,
    users_score_path: str,
    dim_user,
    dim_anime,
    dim_date
):
    """
    –°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—é —Ñ–∞–∫—Ç—ñ–≤ (Fact_UserRatings) –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω–∏–º–∏ –ø–æ–ª—è–º–∏.
    
    Args:
        spark: SparkSession
        users_score_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É users-score-2023.csv
        dim_user: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        dim_anime: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∞–Ω—ñ–º–µ
        dim_date: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –¥–∞—Ç–∏
        
    Returns:
        DataFrame –∑ —Ç–∞–±–ª–∏—Ü–µ—é —Ñ–∞–∫—Ç—ñ–≤
    """
    print("üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Fact_UserRatings...")
    
    # –ó—á–∏—Ç—É—î–º–æ –¥–∞–Ω—ñ –æ—Ü—ñ–Ω–æ–∫
    df_ratings = spark.read.csv(users_score_path, header=True, inferSchema=True)
    
    # –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î–º–æ –∫–æ–ª–æ–Ω–∫–∏
    df_ratings = df_ratings.select(
        col("user_id").alias("User_ID"),
        col("anime_id").alias("Anime_ID"),
        col("rating").alias("User_Rating")
    )
    
    # –û–±'—î–¥–Ω—É—î–º–æ –∑ Dim_User –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è User_SK
    df_ratings = df_ratings.join(
        dim_user.select("User_SK", "User_ID"),
        on="User_ID",
        how="inner"
    )
    
    # –û–±'—î–¥–Ω—É—î–º–æ –∑ Dim_Anime –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è Anime_SK —Ç–∞ Avg_Score
    df_ratings = df_ratings.join(
        dim_anime.select("Anime_SK", "Anime_ID", "Avg_Score"),
        on="Anime_ID",
        how="inner"
    )
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
    df_ratings = df_ratings.withColumn(
        "Rating_Deviation",
        col("User_Rating") - col("Avg_Score")
    )
    
    df_ratings = df_ratings.withColumn(
        "Is_Above_Average",
        when(col("User_Rating") > col("Avg_Score"), 1).otherwise(0)
    )
    
    df_ratings = df_ratings.withColumn(
        "Is_High_Rating",
        when(col("User_Rating") >= 8, 1).otherwise(0)
    )
    
    df_ratings = df_ratings.withColumn(
        "Is_Low_Rating",
        when(col("User_Rating") <= 4, 1).otherwise(0)
    )
    
    df_ratings = df_ratings.withColumn("Rating_Count", lit(1))
    
    # –î–ª—è Date_SK –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∞—Ç—É (–∞–±–æ –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –¥–∞—Ç—É –∑ —ñ–Ω—à–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞)
    # –ü–æ–∫–∏ —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–∞—Ç—É –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º (—Å—å–æ–≥–æ–¥–Ω—ñ)
    from datetime import datetime
    today = datetime.now().strftime("%Y%m%d")
    df_ratings = df_ratings.withColumn("Date_SK", lit(int(today)))
    
    # –í–∏–±–µ—Ä–∞—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
    fact_ratings = df_ratings.select(
        "User_SK",
        "Anime_SK",
        "Date_SK",
        "User_Rating",
        "Rating_Deviation",
        "Is_Above_Average",
        "Is_High_Rating",
        "Is_Low_Rating",
        "Rating_Count"
    )
    
    print(f"‚úÖ Fact_UserRatings —Å—Ç–≤–æ—Ä–µ–Ω–æ: {fact_ratings.count()} –æ—Ü—ñ–Ω–æ–∫")
    return fact_ratings


def create_star_schema(spark: SparkSession, data_path: str = "data"):
    """
    –°—Ç–≤–æ—Ä—é—î –ø–æ–≤–Ω—É –∑—ñ—Ä—á–∞—Å—Ç—É —Å—Ö–µ–º—É –¥–∞–Ω–∏—Ö.
    
    Args:
        spark: SparkSession
        data_path: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ –¥–∞–Ω–∏–º–∏
        
    Returns:
        Tuple –∑ (dim_user, dim_anime, dim_date, fact_ratings)
    """
    print("üåü –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ –¥–∞–Ω–∏—Ö...\n")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏–º—ñ—Ä–∏
    dim_user = create_dim_user(spark, f"{data_path}/users-details-2023.csv")
    dim_anime = create_dim_anime(spark, f"{data_path}/anime-dataset-2023.csv")
    dim_date = create_dim_date(spark)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—é —Ñ–∞–∫—Ç—ñ–≤
    fact_ratings = create_fact_user_ratings(
        spark,
        f"{data_path}/users-score-2023.csv",
        dim_user,
        dim_anime,
        dim_date
    )
    
    print("\n‚úÖ –ó—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞ —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–∞!")
    print("\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ö–µ–º–∏:")
    print(f"  - Dim_User: {dim_user.count()} —Ä—è–¥–∫—ñ–≤")
    print(f"  - Dim_Anime: {dim_anime.count()} —Ä—è–¥–∫—ñ–≤")
    print(f"  - Dim_Date: {dim_date.count()} —Ä—è–¥–∫—ñ–≤")
    print(f"  - Fact_UserRatings: {fact_ratings.count()} —Ä—è–¥–∫—ñ–≤")
    
    return dim_user, dim_anime, dim_date, fact_ratings


def save_star_schema_to_parquet(
    dim_user,
    dim_anime,
    dim_date,
    fact_ratings,
    output_path: str = "data/star_schema"
):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –∑—ñ—Ä—á–∞—Å—Ç—É —Å—Ö–µ–º—É —É —Ñ–æ—Ä–º–∞—Ç—ñ Parquet –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –¥–æ—Å—Ç—É–ø—É.
    
    Args:
        dim_user: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        dim_anime: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –∞–Ω—ñ–º–µ
        dim_date: DataFrame –∑ –≤–∏–º—ñ—Ä–æ–º –¥–∞—Ç–∏
        fact_ratings: DataFrame –∑ —Ç–∞–±–ª–∏—Ü–µ—é —Ñ–∞–∫—Ç—ñ–≤
        output_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    """
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ —É Parquet —Ñ–æ—Ä–º–∞—Ç—ñ –≤ {output_path}...")
    
    dim_user.write.mode("overwrite").parquet(f"{output_path}/dim_user")
    dim_anime.write.mode("overwrite").parquet(f"{output_path}/dim_anime")
    dim_date.write.mode("overwrite").parquet(f"{output_path}/dim_date")
    fact_ratings.write.mode("overwrite").parquet(f"{output_path}/fact_user_ratings")
    
    print("‚úÖ –ó—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É Parquet —Ñ–æ—Ä–º–∞—Ç—ñ!")


def load_star_schema_from_parquet(spark: SparkSession, parquet_path: str = "data/star_schema"):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑—ñ—Ä—á–∞—Å—Ç—É —Å—Ö–µ–º—É –∑ Parquet —Ñ–∞–π–ª—ñ–≤.
    
    Args:
        spark: SparkSession
        parquet_path: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ Parquet —Ñ–∞–π–ª–∞–º–∏
        
    Returns:
        Tuple –∑ (dim_user, dim_anime, dim_date, fact_ratings)
    """
    print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑—ñ—Ä—á–∞—Å—Ç–æ—ó —Å—Ö–µ–º–∏ –∑ Parquet —Ñ–∞–π–ª—ñ–≤ –∑ {parquet_path}...")
    
    dim_user = spark.read.parquet(f"{parquet_path}/dim_user")
    dim_anime = spark.read.parquet(f"{parquet_path}/dim_anime")
    dim_date = spark.read.parquet(f"{parquet_path}/dim_date")
    fact_ratings = spark.read.parquet(f"{parquet_path}/fact_user_ratings")
    
    print("‚úÖ –ó—ñ—Ä—á–∞—Å—Ç–∞ —Å—Ö–µ–º–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ Parquet —Ñ–∞–π–ª—ñ–≤!")
    
    return dim_user, dim_anime, dim_date, fact_ratings
